# VFCMULCPH/VFMULCPH

Complex Multiply FP16 Values

This instruction performs a complex multiply operation.
There are normal and complex conjugate forms of the oper-ation.
The broadcasting and masking for this operation is done on 32-bit quantities representing a pair of FP16 values.Rounding is performed at every FMA (fused multiply and add) boundary.
Execution occurs as if all MXCSR excep-tions are masked.
MXCSR status bits are updated to reflect exceptional conditions.

## Exceptions

- Other Exceptions
  > EVEX-encoded instructions, see Table2-49,
  >  "Type E4 Class Exception Conditions."
- SIMD Floating-Point Exceptions
  > Invalid, Underflow, Overflow, Precision, Denormal

## Operation

```C
VFMULCPH dest{k1}, src1, src2 (AVX512)VL = 128, 256 or 512KL := VL/32FOR i := 0 to KL-1:IF k1[i] or *no writemask*:IF broadcasting and src2 is memory:tsrc2.fp16[2*i+0] := src2.fp16[0]tsrc2.fp16[2*i+1] := src2.fp16[1]ELSE:tsrc2.fp16[2*i+0] := src2.fp16[2*i+0]IF k1[i] or *no writemask*:tmp.fp16[2*i+0] := src1.fp16[2*i+0] * tsrc2.fp16[2*i+0]tmp.fp16[2*i+1] := src1.fp16[2*i+1] * tsrc2.fp16[2*i+0]FOR i := 0 to KL-1:IF k1[i] or *no writemask*:// non-conjugate version subtracts last even termdest.fp16[2*i+0] := tmp.fp16[2*i+0] - src1.fp16[2*i+1] * tsrc2.fp16[2*i+1]dest.fp16[2*i+1] := tmp.fp16[2*i+1] + src1.fp16[2*i+0] * tsrc2.fp16[2*i+1]ELSE IF *zeroing*:dest.fp16[2*i+0] := 0dest.fp16[2*i+1] := 0DEST[MAXVL-1:VL] := 0VFCMULCPH dest{k1}, src1, src2 (AVX512)VL = 128, 256 or 512KL := VL/32FOR i := 0 to KL-1:IF k1[i] or *no writemask*:IF broadcasting and src2 is memory:tsrc2.fp16[2*i+0] := src2.fp16[0]tsrc2.fp16[2*i+1] := src2.fp16[1]ELSE:tsrc2.fp16[2*i+0] := src2.fp16[2*i+0]tsrc2.fp16[2*i+1] := src2.fp16[2*i+1]FOR i := 0 to KL-1:IF k1[i] or *no writemask*:tmp.fp16[2*i+0] := src1.fp16[2*i+0] * tsrc2.fp16[2*i+0]tmp.fp16[2*i+1] := src1.fp16[2*i+1] * tsrc2.fp16[2*i+0]FOR i := 0 to KL-1:IF k1[i] or *no writemask*:// conjugate version subtracts odd final termdest.fp16[2*i] := tmp.fp16[2*i+0] +src1.fp16[2*i+1] * tsrc2.fp16[2*i+1]dest.fp16[2*i+1] := tmp.fp16[2*i+1] - src1.fp16[2*i+0] * tsrc2.fp16[2*i+1]ELSE IF *zeroing*:dest.fp16[2*i+0] := 0dest.fp16[2*i+1] := 0DEST[MAXVL-1:VL] := 0 Intel C/C++ Compiler Intrinsic EquivalentVFCMULCPH __m128h _mm_cmul_pch (__m128h a, __m128h b);VFCMULCPH __m128h _mm_mask_cmul_pch (__m128h src, __mmask8 k, __m128h a, __m128h b);VFCMULCPH __m128h _mm_maskz_cmul_pch (__mmask8 k, __m128h a, __m128h b);VFCMULCPH __m256h _mm256_cmul_pch (__m256h a, __m256h b);VFCMULCPH __m256h _mm256_mask_cmul_pch (__m256h src, __mmask8 k, __m256h a, __m256h b);VFCMULCPH __m256h _mm256_maskz_cmul_pch (__mmask8 k, __m256h a, __m256h b);VFCMULCPH __m512h _mm512_cmul_pch (__m512h a, __m512h b);VFCMULCPH __m512h _mm512_mask_cmul_pch (__m512h src, __mmask16 k, __m512h a, __m512h b);VFCMULCPH __m512h _mm512_cmul_round_pch (__m512h a, __m512h b, const int rounding);VFCMULCPH __m512h _mm512_mask_cmul_round_pch (__m512h src, __mmask16 k, __m512h a, __m512h b, const int rounding);VFCMULCPH __m512h _mm512_maskz_cmul_round_pch (__mmask16 k, __m512h a, __m512h b, const int rounding);VFCMULCPH __m128h _mm_fcmul_pch (__m128h a, __m128h b);VFCMULCPH __m128h _mm_mask_fcmul_pch (__m128h src, __mmask8 k, __m128h a, __m128h b);VFCMULCPH __m128h _mm_maskz_fcmul_pch (__mmask8 k, __m128h a, __m128h b);VFCMULCPH __m256h _mm256_fcmul_pch (__m256h a, __m256h b);VFCMULCPH __m256h _mm256_mask_fcmul_pch (__m256h src, __mmask8 k, __m256h a, __m256h b);VFCMULCPH __m256h _mm256_maskz_fcmul_pch (__mmask8 k, __m256h a, __m256h b);VFCMULCPH __m512h _mm512_fcmul_pch (__m512h a, __m512h b);VFCMULCPH __m512h _mm512_mask_fcmul_pch (__m512h src, __mmask16 k, __m512h a, __m512h b);VFCMULCPH __m512h _mm512_maskz_fcmul_pch (__mmask16 k, __m512h a, __m512h b);VFCMULCPH __m512h _mm512_fcmul_round_pch (__m512h a, __m512h b, const int rounding);VFCMULCPH __m512h _mm512_mask_fcmul_round_pch (__m512h src, __mmask16 k, __m512h a, __m512h b, const int rounding);VFCMULCPH __m512h _mm512_maskz_fcmul_round_pch (__mmask16 k, __m512h a, __m512h b, const int rounding);VFMULCPH __m128h _mm_fmul_pch (__m128h a, __m128h b);VFMULCPH __m128h _mm_mask_fmul_pch (__m128h src, __mmask8 k, __m128h a, __m128h b);VFMULCPH __m128h _mm_maskz_fmul_pch (__mmask8 k, __m128h a, __m128h b);VFMULCPH __m256h _mm256_fmul_pch (__m256h a, __m256h b);VFMULCPH __m256h _mm256_mask_fmul_pch (__m256h src, __mmask8 k, __m256h a, __m256h b);VFMULCPH __m256h _mm256_maskz_fmul_pch (__mmask8 k, __m256h a, __m256h b);VFMULCPH __m512h _mm512_fmul_pch (__m512h a, __m512h b);VFMULCPH __m512h _mm512_mask_fmul_pch (__m512h src, __mmask16 k, __m512h a, __m512h b);VFMULCPH __m512h _mm512_maskz_fmul_pch (__mmask16 k, __m512h a, __m512h b);VFMULCPH __m512h _mm512_fmul_round_pch (__m512h a, __m512h b, const int rounding);VFMULCPH __m512h _mm512_mask_fmul_round_pch (__m512h src, __mmask16 k, __m512h a, __m512h b, const int rounding);VFMULCPH __m512h _mm512_maskz_fmul_round_pch (__mmask16 k, __m512h a, __m512h b, const int rounding);VFMULCPH __m128h _mm_mask_mul_pch (__m128h src, __mmask8 k, __m128h a, __m128h b);VFMULCPH __m128h _mm_maskz_mul_pch (__mmask8 k, __m128h a, __m128h b);VFMULCPH __m128h _mm_mul_pch (__m128h a, __m128h b);VFMULCPH __m256h _mm256_mask_mul_pch (__m256h src, __mmask8 k, __m256h a, __m256h b);VFMULCPH __m256h _mm256_maskz_mul_pch (__mmask8 k, __m256h a, __m256h b);VFMULCPH __m256h _mm256_mul_pch (__m256h a, __m256h b);VFMULCPH __m512h _mm512_mask_mul_pch (__m512h src, __mmask16 k, __m512h a, __m512h b);VFMULCPH __m512h _mm512_maskz_mul_pch (__mmask16 k, __m512h a, __m512h b);VFMULCPH __m512h _mm512_mul_pch (__m512h a, __m512h b);VFMULCPH __m512h _mm512_mask_mul_round_pch (__m512h src, __mmask16 k, __m512h a, __m512h b, const int rounding);VFMULCPH __m512h _mm512_maskz_mul_round_pch (__mmask16 k, __m512h a, __m512h b, const int rounding);VFMULCPH __m512h _mm512_mul_round_pch (__m512h a, __m512h b, const int rounding);
```
