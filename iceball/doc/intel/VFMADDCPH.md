# VFCMADDCPH/VFMADDCPH

Complex Multiply and Accumulate FP16 Values

This instruction performs a complex multiply and accumulate operation.
There are normal and complex conjugate forms of the operation.The broadcasting and masking for this operation is done on 32-bit quantities representing a pair of FP16 values.Rounding is performed at every FMA (fused multiply and add) boundary.
Execution occurs as if all MXCSR excep-

## Exceptions

- Other Exceptions
  > EVEX-encoded instructions, see Table2-49, "Type E4 Class Exception Conditions."
- SIMD Floating-Point Exceptions
  > Invalid, Underflow, Overflow, Precision, Denormal.

## Operation

```C
VFMADDCPH dest{k1}, src1, src2 (AVX512)VL = 128, 256, 512KL := VL / 32FOR i := 0 to KL-1:IF k1[i] or *no writemask*:IF broadcasting and src2 is memory:tsrc2.fp16[2*i+0] := src2.fp16[0]tsrc2.fp16[2*i+1] := src2.fp16[1]ELSE:tsrc2.fp16[2*i+0] := src2.fp16[2*i+0]tsrc2.fp16[2*i+1] := src2.fp16[2*i+1]FOR i := 0 to KL-1:IF k1[i] or *no writemask*:tmp[2*i+0] := dest.fp16[2*i+0] + src1.fp16[2*i+0] * tsrc2.fp16[2*i+0]tmp[2*i+1] := dest.fp16[2*i+1] + src1.fp16[2*i+1] * tsrc2.fp16[2*i+0]FOR i := 0 to KL-1:IF k1[i] or *no writemask*:// non-conjugate version subtracts even termdest.fp16[2*i+0] := tmp[2*i+0] - src1.fp16[2*i+1] * tsrc2.fp16[2*i+1]dest.fp16[2*i+1] := tmp[2*i+1] + src1.fp16[2*i+0] * tsrc2.fp16[2*i+1]ELSE IF *zeroing*:dest.fp16[2*i+0] := 0dest.fp16[2*i+1] := 0DEST[MAXVL-1:VL] := 0 VFCMADDCPH dest{k1}, src1, src2 (AVX512)VL = 128, 256, 512KL := VL / 32FOR i := 0 to KL-1:IF k1[i] or *no writemask*:IF broadcasting and src2 is memory:tsrc2.fp16[2*i+0] := src2.fp16[0]tsrc2.fp16[2*i+1] := src2.fp16[1]ELSE:tsrc2.fp16[2*i+0] := src2.fp16[2*i+0]tsrc2.fp16[2*i+1] := src2.fp16[2*i+1]FOR i := 0 to KL-1:IF k1[i] or *no writemask*:tmp[2*i+0] := dest.fp16[2*i+0] + src1.fp16[2*i+0] * tsrc2.fp16[2*i+0]tmp[2*i+1] := dest.fp16[2*i+1] + src1.fp16[2*i+1] * tsrc2.fp16[2*i+0]FOR i := 0 to KL-1:IF k1[i] or *no writemask*:// conjugate version subtracts odd final termdest.fp16[2*i+0] := tmp[2*i+0] + src1.fp16[2*i+1] * tsrc2.fp16[2*i+1]dest.fp16[2*i+0] := 0dest.fp16[2*i+1] := 0DEST[MAXVL-1:VL] := 0Intel C/C++ Compiler Intrinsic EquivalentVFCMADDCPH __m128h _mm_fcmadd_pch (__m128h a, __m128h b, __m128h c);VFCMADDCPH __m128h _mm_mask_fcmadd_pch (__m128h a, __mmask8 k, __m128h b, __m128h c);VFCMADDCPH __m128h _mm_mask3_fcmadd_pch (__m128h a, __m128h b, __m128h c, __mmask8 k);VFCMADDCPH __m128h _mm_maskz_fcmadd_pch (__mmask8 k, __m128h a, __m128h b, __m128h c);VFCMADDCPH __m256h _mm256_fcmadd_pch (__m256h a, __m256h b, __m256h c);VFCMADDCPH __m256h _mm256_mask_fcmadd_pch (__m256h a, __mmask8 k, __m256h b, __m256h c);VFCMADDCPH __m256h _mm256_mask3_fcmadd_pch (__m256h a, __m256h b, __m256h c, __mmask8 k);VFCMADDCPH __m256h _mm256_maskz_fcmadd_pch (__mmask8 k, __m256h a, __m256h b, __m256h c);VFCMADDCPH __m512h _mm512_fcmadd_pch (__m512h a, __m512h b, __m512h c);VFCMADDCPH __m512h _mm512_mask_fcmadd_pch (__m512h a, __mmask16 k, __m512h b, __m512h c);VFCMADDCPH __m512h _mm512_mask3_fcmadd_pch (__m512h a, __m512h b, __m512h c, __mmask16 k);VFCMADDCPH __m512h _mm512_maskz_fcmadd_pch (__mmask16 k, __m512h a, __m512h b, __m512h c);VFCMADDCPH __m512h _mm512_fcmadd_round_pch (__m512h a, __m512h b, __m512h c, const int rounding);VFCMADDCPH __m512h _mm512_mask_fcmadd_round_pch (__m512h a, __mmask16 k, __m512h b, __m512h c, const int rounding);VFCMADDCPH __m512h _mm512_mask3_fcmadd_round_pch (__m512h a, __m512h b, __m512h c, __mmask16 k, const int rounding);VFCMADDCPH __m512h _mm512_maskz_fcmadd_round_pch (__mmask16 k, __m512h a, __m512h b, __m512h c, const int rounding);VFMADDCPH __m128h _mm_fmadd_pch (__m128h a, __m128h b, __m128h c);VFMADDCPH __m128h _mm_mask_fmadd_pch (__m128h a, __mmask8 k, __m128h b, __m128h c);VFMADDCPH __m128h _mm_mask3_fmadd_pch (__m128h a, __m128h b, __m128h c, __mmask8 k);VFMADDCPH __m128h _mm_maskz_fmadd_pch (__mmask8 k, __m128h a, __m128h b, __m128h c);VFMADDCPH __m256h _mm256_fmadd_pch (__m256h a, __m256h b, __m256h c);VFMADDCPH __m256h _mm256_mask_fmadd_pch (__m256h a, __mmask8 k, __m256h b, __m256h c);VFMADDCPH __m256h _mm256_mask3_fmadd_pch (__m256h a, __m256h b, __m256h c, __mmask8 k);VFMADDCPH __m256h _mm256_maskz_fmadd_pch (__mmask8 k, __m256h a, __m256h b, __m256h c);VFMADDCPH __m512h _mm512_fmadd_pch (__m512h a, __m512h b, __m512h c);VFMADDCPH __m512h _mm512_mask_fmadd_pch (__m512h a, __mmask16 k, __m512h b, __m512h c);VFMADDCPH __m512h _mm512_mask3_fmadd_pch (__m512h a, __m512h b, __m512h c, __mmask16 k);VFMADDCPH __m512h _mm512_maskz_fmadd_pch (__mmask16 k, __m512h a, __m512h b, __m512h c);VFMADDCPH __m512h _mm512_fmadd_round_pch (__m512h a, __m512h b, __m512h c, const int rounding);VFMADDCPH __m512h _mm512_mask_fmadd_round_pch (__m512h a, __mmask16 k, __m512h b, __m512h c, const int rounding);VFMADDCPH __m512h _mm512_mask3_fmadd_round_pch (__m512h a, __m512h b, __m512h c, __mmask16 k, const int rounding);VFMADDCPH __m512h _mm512_maskz_fmadd_round_pch (__mmask16 k, __m512h a, __m512h b, __m512h c, const int rounding);
```
