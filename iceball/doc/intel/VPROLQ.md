# VPROLD/VPROLVD/VPROLQ/VPROLVQ

Bit Rotate Left

Instructionbit Mode Feature SupportFlagBV/VAVX512VLRotate doublewords in xmm2 left by count in the EVEX.128.66.0F38.W0 15 /rAVX512Fcorresponding element of xmm3/m128/m32bcst.
VPROLVD xmm1 {k1}{z}, xmm2, Result written to xmm1 under writemask k1.xmm3/m128/m32bcstEVEX.128.66.0F.W0 72 /1 ibAV/VAVX512VLRotate doublewords in xmm2/m128/m32bcst left VPROLD xmm1 {k1}{z}, AVX512Fby imm8.
Result written to xmm1 using xmm2/m128/m32bcst, imm8writemask k1.BV/VAVX512VLRotate quadwords in xmm2 left by count in the EVEX.128.66.0F38.W1 15 /rAVX512Fcorresponding element of xmm3/m128/m64bcst.
VPROLVQ xmm1 {k1}{z}, xmm2, Result written to xmm1 under writemask k1.xmm3/m128/m64bcstAV/VAVX512VLRotate quadwords in xmm2/m128/m64bcst left EVEX.128.66.0F.W1 72 /1 ibAVX512Fby imm8.
Result written to xmm1 using VPROLQ xmm1 {k1}{z}, writemask k1.xmm2/m128/m64bcst, imm8EVEX.256.66.0F38.W0 15 /rBV/VAVX512VLRotate doublewords in ymm2 left by count in the VPROLVD ymm1 {k1}{z}, ymm2, AVX512Fcorresponding element of ymm3/m256/m32bcst.
ymm3/m256/m32bcstResult written to ymm1 under writemask k1.EVEX.256.66.0F.W0 72 /1 ibAV/VAVX512VLRotate doublewords in ymm2/m256/m32bcst left VPROLD ymm1 {k1}{z}, AVX512Fby imm8.
Result written to ymm1 using ymm2/m256/m32bcst, imm8writemask k1.EVEX.256.66.0F38.W1 15 /rBV/VAVX512VLRotate quadwords in ymm2 left by count in the VPROLVQ ymm1 {k1}{z}, ymm2, AVX512Fcorresponding element of ymm3/m256/m64bcst.
ymm3/m256/m64bcstResult written to ymm1 under writemask k1.EVEX.256.66.0F.W1 72 /1 ibAV/VAVX512VLRotate quadwords in ymm2/m256/m64bcst left VPROLQ ymm1 {k1}{z}, AVX512Fby imm8.
Result written to ymm1 using ymm2/m256/m64bcst, imm8writemask k1.EVEX.512.66.0F38.W0 15 /rBV/VAVX512FRotate left of doublewords in zmm2 by count in VPROLVD zmm1 {k1}{z}, zmm2, the corresponding element of zmm3/m512/m32bcstzmm3/m512/m32bcst.
Result written to zmm1 using writemask k1.EVEX.512.66.0F.W0 72 /1 ibAV/VAVX512FRotate left of doublewords in VPROLD zmm1 {k1}{z}, zmm3/m512/m32bcst by imm8.
Result written to zmm2/m512/m32bcst, imm8zmm1 using writemask k1.EVEX.512.66.0F38.W1 15 /rBV/VAVX512FRotate quadwords in zmm2 left by count in the VPROLVQ zmm1 {k1}{z}, zmm2, corresponding element of zmm3/m512/m64bcst.
zmm3/m512/m64bcstResult written to zmm1under writemask k1.AV/VAVX512FRotate quadwords in zmm2/m512/m64bcst left EVEX.512.66.0F.W1 72 /1 ibby imm8.
Result written to zmm1 using VPROLQ zmm1 {k1}{z}, writemask k1.zmm2/m512/m64bcst, imm8Instruction Operand EncodingOp/EnTuple TypeOperand 1Operand 2Operand 3Operand 4Rotates the bits in the individual data elements (doublewords, or quadword) in the first source operand to the left by the number of bits specified in the count operand.
If the value specified by the count operand is greater than 31 (for doublewords), or 63 (for a quadword), then the count operand modulo the data size (32 or 64) is used.
EVEX.128 encoded version: The destination operand is a XMM register.
The source operand is a XMM register or a memory location (for immediate form).
The count operand can come either from an XMM register or a memory location or an 8-bit immediate.
Bits (MAXVL-1:128) of the corresponding ZMM register are zeroed.EVEX.256 encoded version: The destination operand is a YMM register.
The source operand is a YMM register or a memory location (for immediate form).
The count operand can come either from an XMM register or a memory location or an 8-bit immediate.
Bits (MAXVL-1:256) of the corresponding ZMM register are zeroed.EVEX.512 encoded version: The destination operand is a ZMM register updated according to the writemask.
For the count operand in immediate form, the source operand can be a ZMM register, a 512-bit memory location or a 512-bit vector broadcasted from a 32/64-bit memory location, the count operand is an 8-bit immediate.
For the count operand in variable form, the first source operand (the second operand) is a ZMM register and the counter operand (the third operand) is a ZMM register, a 512-bit memory location or a 512-bit vector broadcasted from a 32/64-bit memory location.


## Exceptions

- SIMD Floating-Point Exceptions
  > None.

## Operation

```C
LEFT_ROTATE_DWORDS(SRC, COUNT_SRC)COUNT := COUNT_SRC modulo 32;DEST[31:0] := (SRC << COUNT) | (SRC >> (32 - COUNT));LEFT_ROTATE_QWORDS(SRC, COUNT_SRC)COUNT := COUNT_SRC modulo 64;DEST[63:0] := (SRC << COUNT) | (SRC >> (64 - COUNT));VPROLD (EVEX encoded versions)(KL, VL) = (4, 128), (8, 256), (16, 512)FOR j := 0 TO KL-1i := j * 32IF k1[j] OR *no writemask* THENIF (EVEX.b = 1) AND (SRC1 *is memory*)THEN DEST[i+31:i] := LEFT_ROTATE_DWORDS(SRC1[31:0], imm8)ELSE DEST[i+31:i] := LEFT_ROTATE_DWORDS(SRC1[i+31:i], imm8)FI;ELSE IF *merging-masking*; merging-maskingTHEN *DEST[i+31:i] remains unchanged*ELSE *zeroing-masking*; zeroing-masking DEST[i+31:i] := 0FIFI;ENDFORDEST[MAXVL-1:VL] := 0VPROLVD (EVEX encoded versions)(KL, VL) = (4, 128), (8, 256), (16, 512)FOR j := 0 TO KL-1i := j * 32IF k1[j] OR *no writemask* THENIF (EVEX.b = 1) AND (SRC2 *is memory*)THEN DEST[i+31:i] := LEFT_ROTATE_DWORDS(SRC1[i+31:i], SRC2[31:0])ELSE DEST[i+31:i] := LEFT_ROTATE_DWORDS(SRC1[ELSE IF *merging-masking*; merging-maskingTHEN *DEST[i+31:i] remains unchanged*ELSE *zeroing-masking*; zeroing-masking DEST[i+31:i] := 0FIFI;ENDFORDEST[MAXVL-1:VL] := 0VPROLQ (EVEX encoded versions)(KL, VL) = (2, 128), (4, 256), (8, 512)FOR j := 0 TO KL-1i := j * 64IF k1[j] OR *no writemask* THENIF (EVEX.b = 1) AND (SRC1 *is memory*)THEN DEST[i+63:i] := LEFT_ROTATE_QWORDS(SRC1[63:0], imm8)ELSE DEST[i+63:i] := LEFT_ROTATE_QWORDS(SRC1[i+63:i], imm8)FI;ELSE IF *merging-masking*; merging-maskingTHEN *DEST[i+63:i] remains unchanged*ELSE *zeroing-masking*; zeroing-masking DEST[i+63:i] := 0FIFI;ENDFORDEST[MAXVL-1:VL] := 0VPROLVQ (EVEX encoded versions)(KL, VL) = (2, 128), (4, 256), (8, 512)FOR j := 0 TO KL-1i := j * 64IF k1[j] OR *no writemask* THENIF (EVEX.b = 1) AND (SRC2 *is memory*)THEN DEST[i+63:i] := LEFT_ROTATE_QWORDS(SRC1[i+63:i], SRC2[63:0])ELSE DEST[i+63:i] := LEFT_ROTATE_QWORDS(SRC1[i+63:i], SRC2[i+63:i])FI;ELSE IF *merging-masking*; merging-maskingTHEN *DEST[i+63:i] remains unchanged*ELSE *zeroing-masking*; zeroing-masking DEST[i+63:i] := 0FIFI;Intel C/C++ Compiler Intrinsic EquivalentVPROLD __m512i _mm512_rol_epi32(__m512i a, int imm);VPROLD __m512i _mm512_mask_rol_epi32(__m512i a, __mmask16 k, __m512i b, int imm);VPROLD __m512i _mm512_maskz_rol_epi32( __mmask16 k, __m512i a, int imm);VPROLD __m256i _mm256_rol_epi32(__m256i a, int imm);VPROLD __m256i _mm256_mask_rol_epi32(__m256i a, __mmask8 k, __m256i b, int imm);VPROLD __m256i _mm256_maskz_rol_epi32( __mmask8 k, __m256i a, int imm);VPROLD __m128i _mm_rol_epi32(__m128i a, int imm);VPROLD __m128i _mm_mask_rol_epi32(__m128i a, __mmask8 k, __m128i b, int imm);VPROLD __m128i _mm_maskz_rol_epi32( __mmask8 k, __m128i a, int imm);VPROLQ __m512i _mm512_rol_epi64(__m512i a, int imm);VPROLQ __m512i _mm512_mask_rol_epi64(__m512i a, __mmask8 k, __m512i b, int imm);VPROLQ __m512i _mm512_maskz_rol_epi64(__mmask8 k, __m512i a, int imm);VPROLQ __m256i _mm256_rol_epi64(__m256i a, int imm);VPROLQ __m256i _mm256_mask_rol_epi64(__m256i a, __mmask8 k, __m256i b, int imm);VPROLQ __m256i _mm256_maskz_rol_epi64( __mmask8 k, __m256i a, int imm);VPROLQ __m128i _mm_rol_epi64(__m128i a, int imm);VPROLQ __m128i _mm_mask_rol_epi64(__m128i a, __mmask8 k, __m128i b, int imm);VPROLQ __m128i _mm_maskz_rol_epi64( __mmask8 k, __m128i a, int imm);VPROLVD __m512i _mm512_rolv_epi32(__m512i a, __m512i cnt);VPROLVD __m512i _mm512_mask_rolv_epi32(__m512i a, __mmask16 k, __m512i b, __m512i cnt);VPROLVD __m512i _mm512_maskz_rolv_epi32(__mmask16 k, __m512i a, __m512i cnt);VPROLVD __m256i _mm256_rolv_epi32(__m256i a, __m256i cnt);VPROLVD __m256i _mm256_mask_rolv_epi32(__m256i a, __mmask8 k, __m256i b, __m256i cnt);VPROLVD __m256i _mm256_maskz_rolv_epi32(__mmask8 k, __m256i a, __m256i cnt);VPROLVD __m128i _mm_rolv_epi32(__m128i a, __m128i cnt);VPROLVD __m128i _mm_mask_rolv_epi32(__m128i a, __mmask8 k, __m128i b, __m128i cnt);VPROLVD __m128i _mm_maskz_rolv_epi32(__mmask8 k, __m128i a, __m128i cnt);VPROLVQ __m512i _mm512_rolv_epi64(__m512i a, __m512i cnt);VPROLVQ __m512i _mm512_mask_rolv_epi64(__m512i a, __mmask8 k, __m512i b, __m512i cnt);VPROLVQ __m512i _mm512_maskz_rolv_epi64( __mmask8 k, __m512i a, __m512i cnt);VPROLVQ __m256i _mm256_rolv_epi64(__m256i a, __m256i cnt);VPROLVQ __m256i _mm256_mask_rolv_epi64(__m256i a, __mmask8 k, __m256i b, __m256i cnt);VPROLVQ __m256i _mm256_maskz_rolv_epi64(__mmask8 k, __m256i a, __m256i cnt);VPROLVQ __m128i _mm_rolv_epi64(__m128i a, __m128i cnt);VPROLVQ __m128i _mm_mask_rolv_epi64(__m128i a, __mmask8 k, __m128i b, __m128i cnt);VPROLVQ __m128i _mm_maskz_rolv_epi64(__mmask8 k, __m128i a, __m128i cnt);
```
