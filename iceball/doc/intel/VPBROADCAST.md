# VPBROADCAST

Load Integer and Broadcast

Load integer data from the source operand (the second operand) and broadcast to all elements of the destination operand (the first operand).VEX256-encoded VPBROADCASTB/W/D/Q: The source operand is 8-bit, 16-bit, 32-bit, 64-bit memory location or the low 8-bit, 16-bit 32-bit, 64-bit data in an XMM register.
The destination operand is a YMM register.
VPBROAD-CASTI128 support the source operand of 128-bit memory EVEX-encoded VPBROADCASTD/Q: The source operand is a 32-bit, 64-bit memory location or the low 32-bit, 64-bit data in an XMM register.
The destination operand is a ZMM/YMM/XMM register and updated according to the writemask k1.
VPBROADCASTI32X4 and VPBROADCASTI64X4: The destination operand is a ZMM register and updated according to the writemask k1.
The source operand is 128-bit or 256-bit memory location.
Register source encodings for VBROADCASTI32X4 and VBROADCASTI64X4 are reserved and will #UD.Note: VEX.vvvv and EVEX.vvvv are reserved and must be 1111b otherwise instructions will #UD.If VPBROADCASTI128 is encoded with VEX.L= 0, an attempt to execute the instruction encoded with VEX.L= 0 will cause an #UD exception.X0X0X0X0X0X0X0X0X0m32DESTFigure 5-16.
 VPBROADCASTD Operation (VEX.256 encoded version)X0X0X0X00X0000m32DESTFigure 5-17.
 VPBROADCASTD Operation (128-bit version)m64X0DESTX0X0X0m128X0DESTX0X0Figure 5-19.
 VBROADCASTI128 Operation (256-bit version)m256X0DESTX0X0Figure 5-20.
 VBROADCASTI256 Operation (512-bit version)

## Exceptions

- Other Exceptions
  > EVEX-encoded instructions, see Table2-23, "Type 6 Class Exception Conditions."
  > EVEX-encoded instructions, syntax with reg/mem operand,
  >  see Table2-53, "Type E6 Class Exception Conditions."
  > Additionally:
  - #UD - If VEX.L = 0 for VPBROADCASTQ, VPBROADCASTI128.
  > If EVEX.L'L = 0 for 
  > VBROADCASTI32X4
  > /
  > VBROADCASTI64X2
  > .
  > If EVEX.L'L < 10b for 
  > VBROADCASTI32X8
  > /
- SIMD Floating-Point Exceptions
  > None.

## Operation

```C
VPBROADCASTB (EVEX encoded versions)(KL, VL) = (16, 128), (32, 256), (64, 512)FOR j := 0 TO KL-1i := j * 8IF k1[j] OR *no writemask*THEN DEST[i+7:i] := SRC[7:0]ELSE IF *merging-masking*; merging-maskingTHEN *DEST[i+7:i] remains unchanged*ELSE ; zeroing-maskingDEST[i+7:i] := 0FIFI;VPBROADCASTW (EVEX encoded versions)(KL, VL) = (8, 128), (16, 256), (32, 512)FOR j := 0 TO KL-1i := j * 16IF k1[j] OR *no writemask*THEN DEST[i+15:i] := SRC[15:0]ELSE IF *merging-masking*; merging-maskingTHEN *DEST[i+15:i] remains unchanged*ELSE ; zeroing-maskingDEST[i+15:i] := 0FIFI;ENDFORDEST[MAXVL-1:VL] := 0VPBROADCASTD (128 bit version)temp := SRC[31:0]DEST[31:0] := tempDEST[63:32] := tempDEST[95:64] := tempDEST[127:96] := tempDEST[MAXVL-1:128] := 0VPBROADCASTD (VEX.256 encoded version)temp := SRC[31:0]DEST[31:0] := tempDEST[63:32] := tempDEST[95:64] := tempDEST[127:96] := tempDEST[159:128] := tempDEST[191:160] := tempDEST[223:192] := tempDEST[255:224] := tempDEST[MAXVL-1:256] := 0VPBROADCASTD (EVEX encoded versions)(KL, VL) = (4, 128), (8, 256), (16, 512)FOR j := 0 TO KL-1i := j * 32IF k1[j] OR *no writemask*THEN DEST[i+31:i] := SRC[31:0]ELSE IF *merging-masking*; merging-maskingTHEN *DEST[i+31:i] remains unchanged*ELSE ; zeroing-maskingDEST[i+31:i] := 0FIFI;VPBROADCASTQ (VEX.256 encoded version)temp := SRC[63:0]DEST[63:0] := tempDEST[127:64] := tempDEST[191:128] := tempDEST[255:192] := tempDEST[MAXVL-1:256] := 0VPBROADCASTQ (EVEX encoded versions)(KL, VL) = (2, 128), (4, 256), (8, 512)FOR j := 0 TO KL-1i := j * 64IF k1[j] OR *no writemask*THEN DEST[i+63:i] := SRC[63:0]ELSE IF *merging-masking*; merging-maskingTHEN *DEST[i+63:i] remains unchanged*ELSE ; zeroing-maskingDEST[i+63:i] := 0FIFI;ENDFORDEST[MAXVL-1:VL] := 0VBROADCASTI32x2 (EVEX encoded versions)(KL, VL) = (4, 128), (8, 256), (16, 512)FOR j := 0 TO KL-1i := j * 32n := (j mod 2) * 32IF k1[j] OR *no writemask*THEN DEST[i+31:i] := SRC[n+31:n]ELSE IF *merging-masking*; merging-maskingTHEN *DEST[i+31:i] remains unchanged*ELSE ; zeroing-maskingDEST[i+31:i] := 0FIFI;ENDFORDEST[MAXVL-1:VL] := 0VBROADCASTI128 (VEX.256 encoded version)temp := SRC[127:0]DEST[127:0] := tempVBROADCASTI32X4 (EVEX encoded versions)(KL, VL) = (8, 256), (16, 512)FOR j := 0 TO KL-1i := j* 32n := (j modulo 4) * 32IF k1[j] OR *no writemask*THEN DEST[i+31:i] := SRC[n+31:n]ELSE IF *merging-masking*; merging-maskingTHEN *DEST[i+31:i] remains unchanged*ELSE ; zeroing-maskingDEST[i+31:i] := 0FIFI;ENDFORDEST[MAXVL-1:VL] := 0VBROADCASTI64X2 (EVEX encoded versions)(KL, VL) = (8, 256), (16, 512)FOR j := 0 TO KL-1i := j * 64n := (j modulo 2) * 64IF k1[j] OR *no writemask*THEN DEST[i+63:i] := SRC[n+63:n]ELSE IF *merging-masking*; merging-maskingTHEN *DEST[i+63:i] remains unchanged*ELSE ; zeroing-maskingDEST[i+63:i] = 0FIFI;ENDFOR;VBROADCASTI32X8 (EVEX.U1.512 encoded version)FOR j := 0 TO 15i := j * 32n := (j modulo 8) * 32IF k1[j] OR *no writemask*THEN DEST[i+31:i] := SRC[n+31:n]ELSE IF *merging-masking*; merging-maskingTHEN *DEST[i+31:i] remains unchanged*ELSE ; zeroing-maskingDEST[i+31:i] := 0FIFI;VBROADCASTI64X4 (EVEX.512 encoded version)FOR j := 0 TO 7i := j * 64n := (j modulo 4) * 64IF k1[j] OR *no writemask*THEN DEST[i+63:i] := SRC[n+63:n]ELSE IF *merging-masking*; merging-maskingTHEN *DEST[i+63:i] remains unchanged*ELSE ; zeroing-maskingDEST[i+63:i] := 0FIFI;ENDFORDEST[MAXVL-1:VL] := 0Intel C/C++ Compiler Intrinsic EquivalentVPBROADCASTB __m512i _mm512_broadcastb_epi8( __m128i a);VPBROADCASTB __m512i _mm512_mask_broadcastb_epi8(__m512i s, __mmask64 k, __m128i a);VPBROADCASTB __m512i _mm512_maskz_broadcastb_epi8( __mmask64 k, __m128i a);VPBROADCASTB __m256i _mm256_broadcastb_epi8(__m128i a);VPBROADCASTB __m256i _mm256_mask_broadcastb_epi8(__m256i s, __mmask32 k, __m128i a);VPBROADCASTB __m256i _mm256_maskz_broadcastb_epi8( __mmask32 k, __m128i a);VPBROADCASTB __m128i _mm_mask_broadcastb_epi8(__m128i s, __mmask16 k, __m128i a);VPBROADCASTB __m128i _mm_maskz_broadcastb_epi8( __mmask16 k, __m128i a);VPBROADCASTB __m128i _mm_broadcastb_epi8(__m128i a);VPBROADCASTD __m512i _mm512_broadcastd_epi32( __m128i a);VPBROADCASTD __m512i _mm512_mask_broadcastd_epi32(__m512i s, __mmask16 k, __m128i a);VPBROADCASTD __m512i _mm512_maskz_broadcastd_epi32( __mmask16 k, __m128i a);VPBROADCASTD __m256i _mm256_broadcastd_epi32( __m128i a);VPBROADCASTD __m256i _mm256_mask_broadcastd_epi32(__m256i s, __mmask8 k, __m128i a);VPBROADCASTD __m256i _mm256_maskz_broadcastd_epi32( __mmask8 k, __m128i a);VPBROADCASTD __m128i _mm_broadcastd_epi32(__m128i a);VPBROADCASTD __m128i _mm_mask_broadcastd_epi32(__m128i s, __mmask8 k, __m128i a);VPBROADCASTD __m128i _mm_maskz_broadcastd_epi32( __mmask8 k, __m128i a);VPBROADCASTQ __m512i _mm512_broadcastq_epi64( __m128i a);VPBROADCASTQ __m512i _mm512_mask_broadcastq_epi64(__m512i s, __mmask8 k, __m128i a);VPBROADCASTQ __m512i _mm512_maskz_broadcastq_epi64( __mmask8 k, __m128i a);VPBROADCASTQ __m256i _mm256_broadcastq_epi64(__m128i a);VPBROADCASTQ __m256i _mm256_mask_broadcastq_epi64(__m256i s, __mmask8 k, __m128i a);VPBROADCASTQ __m256i _mm256_maskz_broadcastq_epi64( __mmask8 k, __m128i a);VPBROADCASTQ __m128i _mm_broadcastq_epi64(__m128i a);VPBROADCASTQ __m128i _mm_mask_broadcastq_epi64(__m128i s, __mmask8 k, __m128i a);VPBROADCASTQ __m128i _mm_maskz_broadcastq_epi64( __mmask8 k, __m128i a);VPBROADCASTW __m512i _mm512_broadcastw_epi16(__m128i a);VPBROADCASTW __m512i _mm512_mask_broadcastw_epi16(__m512i s, __mmask32 k, __m128i a);VPBROADCASTW __m512i _mm512_maskz_broadcastw_epi16( __mmask32 k, __m128i a);VPBROADCASTW __m256i _mm256_broadcastw_epi16(__m128i a);VPBROADCASTW __m256i _mm256_mask_broadcastw_epi16(__m256i s, __mmask16 k, __m128i a);VPBROADCASTW __m256i _mm256_maskz_broadcastw_epi16( __mmask16 k, __m128i a);VPBROADCASTW __m128i _mm_broadcastw_epi16(__m128i a);VPBROADCASTW __m128i _mm_mask_broadcastw_epi16(__m128i s, __mmask8 k, __m128i a);VPBROADCASTW __m128i _mm_maskz_broadcVBROADCASTI32x2 __m512i _mm512_mask_broadcast_i32x2(__m512i s, __mmask16 k, __m128i a);VBROADCASTI32x2 __m512i _mm512_maskz_broadcast_i32x2( __mmask16 k, __m128i a);VBROADCASTI32x2 __m256i _mm256_broadcast_i32x2( __m128i a);VBROADCASTI32x2 __m256i _mm256_mask_broadcast_i32x2(__m256i s, __mmask8 k, __m128i a);VBROADCASTI32x2 __m256i _mm256_maskz_broadcast_i32x2( __mmask8 k, __m128i a);VBROADCASTI32x2 __m128i _mm_broadcast_i32x2(__m128i a);VBROADCASTI32x2 __m128i _mm_mask_broadcast_i32x2(__m128i s, __mmask8 k, __m128i a);VBROADCASTI32x2 __m128i _mm_maskz_broadcast_i32x2( __mmask8 k, __m128i a);VBROADCASTI32x4 __m512i _mm512_broadcast_i32x4( __m128i a);VBROADCASTI32x4 __m512i _mm512_mask_broadcast_i32x4(__m512i s, __mmask16 k, __m128i a);VBROADCASTI32x4 __m512i _mm512_maskz_broadcast_i32x4( __mmask16 k, __m128i a);VBROADCASTI32x4 __m256i _mm256_broadcast_i32x4( __m128i a);VBROADCASTI32x4 __m256i _mm256_mask_broadcast_i32x4(__m256i s, __mmask8 k, __m128i a);VBROADCASTI32x4 __m256i _mm256_maskz_broadcast_i32x4( __mmask8 k, __m128i a);VBROADCASTI32x8 __m512i _mm512_broadcast_i32x8( __m256i a);VBROADCASTI32x8 __m512i _mm512_mask_broadcast_i32x8(__m512i s, __mmask16 k, __m256i a);VBROADCASTI32x8 __m512i _mm512_maskz_broadcast_i32x8( __mmask16 k, __m256i a);VBROADCASTI64x2 __m512i _mm512_broadcast_i64x2( __m128i a);VBROADCASTI64x2 __m512i _mm512_mask_broadcast_i64x2(__m512i s, __mmask8 k, __m128i a);VBROADCASTI64x2 __m512i _mm512_maskz_broadcast_i64x2( __mmask8 k, __m128i a);VBROADCASTI64x2 __m256i _mm256_broadcast_i64x2( __m128i a);VBROADCASTI64x2 __m256i _mm256_mask_broadcast_i64x2(__m256i s, __mmask8 k, __m128i a);VBROADCASTI64x2 __m256i _mm256_maskz_broadcast_i64x2( __mmask8 k, __m128i a);VBROADCASTI64x4 __m512i _mm512_broadcast_i64x4( __m256i a);VBROADCASTI64x4 __m512i _mm512_mask_broadcast_i64x4(__m512i s, __mmask8 k, __m256i a);VBROADCASTI64x4 __m512i _mm512_maskz_broadcast_i64x4( __mmask8 k, __m256i a);
```
